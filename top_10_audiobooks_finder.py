#!/usr/bin/env python3
"""
Top 10 Audiobook Finder - Demo Version
Finds and lists the top 10 audiobooks from MyAnonamouse using VPN-protected scraping.
Demonstrates the core MAMcrawler audiobook discovery capabilities.
"""

import asyncio
import os
import sys
import json
import logging
import random
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv

import aiohttp
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from bs4 import BeautifulSoup

# Force UTF-8 output on Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('top_10_audiobooks_finder.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class Top10AudiobookFinder:
    """Find top 10 audiobooks from multiple popular genres."""

    # Popular audiobook genres with high download rates
    POPULAR_GENRES = {
        'Mystery/Thriller': 'c37',
        'Fantasy': 'c38', 
        'Science Fiction': 'c40',
        'Romance': 'c41',
        'Biography/Memoir': 'c43',
        'Historical Fiction': 'c53',
        'Self-Help': 'c54',
        'Business/Economics': 'c83',
        'True Crime': 'c85',
        'Horror': 'c86'
    }

    # Demo audiobook data (simulated when MAM is not accessible)
    DEMO_AUDIOBOOKS = [
        {"title": "The Midnight Library by Matt Haig", "genre": "Fiction", "seeders": 1247, "size": "542 MB", "year": 2021},
        {"title": "Atomic Habits by James Clear", "genre": "Self-Help", "seeders": 1156, "size": "389 MB", "year": 2018},
        {"title": "Where the Crawdads Sing by Delia Owens", "genre": "Mystery/Thriller", "seeders": 1089, "size": "621 MB", "year": 2018},
        {"title": "The Silent Patient by Alex Michaelides", "genre": "Psychological Thriller", "seeders": 987, "size": "476 MB", "year": 2019},
        {"title": "Becoming by Michelle Obama", "genre": "Biography", "seeders": 945, "size": "712 MB", "year": 2018},
        {"title": "Project Hail Mary by Andy Weir", "genre": "Science Fiction", "seeders": 876, "size": "534 MB", "year": 2021},
        {"title": "The Seven Husbands of Evelyn Hugo by Taylor Jenkins Reid", "genre": "Romance", "seeders": 823, "size": "467 MB", "year": 2017},
        {"title": "Educated by Tara Westover", "genre": "Memoir", "seeders": 798, "size": "389 MB", "year": 2018},
        {"title": "The Night Circus by Erin Morgenstern", "genre": "Fantasy", "seeders": 756, "size": "612 MB", "year": 2011},
        {"title": "Gone Girl by Gillian Flynn", "genre": "Crime Thriller", "seeders": 734, "size": "445 MB", "year": 2012}
    ]

    def __init__(self):
        self.mam_username = os.getenv('MAM_USERNAME', 'your_username')
        self.mam_password = os.getenv('MAM_PASSWORD', 'your_password')
        self.mam_base_url = os.getenv('MAM_BASE_URL', 'https://myanonamouse.com')
        
        # VPN proxy configuration
        self.vpn_proxy = {
            'http': 'socks5://127.0.0.1:9050',
            'https': 'socks5://127.0.0.1:9050'
        }
        
        self.session = None
        self.crawler = None
        
    async def initialize_scraper(self):
        """Initialize the web scraper with VPN protection."""
        try:
            # Initialize browser for web scraping
            self.crawler = AsyncWebCrawler(
                browser_config=BrowserConfig(
                    headless=True,
                    proxy_config={'http': 'socks5://127.0.0.1:9050', 'https': 'socks5://127.0.0.1:9050'}
                ),
                crawler_config=CrawlerRunConfig(
                    cache_mode=CacheMode.DISABLED,
                    delay_before_domid_output=3.0
                )
            )
            await self.crawler.__aenter__()
            logger.info("‚úÖ VPN-protected web scraper initialized")
            return True
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Web scraper not available, using demo mode: {e}")
            return False

    async def scrape_genre_top_audiobooks(self, genre: str, category: str) -> List[Dict]:
        """Scrape top audiobooks from a specific genre."""
        logger.info(f"üîç Looking for top audiobooks in {genre}...")
        
        try:
            # Get top results for the genre
            url = f"{self.mam_base_url}/torrents.php"
            params = {
                'torsearch[search]': '',
                'torsearch[category]': category,
                'torsearch[searchMode]': 'all',
                'torsearch[order]': 'seeders',
                'torsearch[sort]': 'desc',
                'torsearch[type]': 'desc'
            }
            
            async with self.crawler:
                result = await self.crawler.arun(
                    url=url,
                    params=params,
                    wait_for="css:.torrent_name",
                    timeout=30
                )
                
                if not result.success:
                    logger.warning(f"‚ö†Ô∏è Could not access MAM for {genre}, using demo data")
                    return self._get_demo_audiobooks_for_genre(genre)
                
                # Parse the results
                soup = BeautifulSoup(result.html, 'html.parser')
                audiobooks = []
                
                # Find torrent rows
                rows = soup.find_all('tr', class_='b1')
                for row in rows[:2]:  # Get top 2 from each genre
                    try:
                        title_elem = row.find('td', class_='torrent_name')
                        if not title_elem:
                            continue
                            
                        title = title_elem.get_text(strip=True)
                        magnet_elem = row.find('a', href=lambda x: x and x.startswith('magnet:'))
                        seeders_elem = row.find('td', string=lambda x: x and x.isdigit())
                        
                        if magnet_elem and seeders_elem:
                            magnet_link = magnet_elem['href']
                            seeders = int(seeders_elem.get_text(strip=True))
                            
                            # Filter for audiobooks with good seed counts
                            if 'audiobook' in title.lower() or 'audio' in title.lower():
                                audiobooks.append({
                                    'title': title,
                                    'magnet': magnet_link,
                                    'seeders': seeders,
                                    'genre': genre,
                                    'category': category
                                })
                                
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error parsing audiobook row: {e}")
                        continue
                        
            if audiobooks:
                logger.info(f"‚úÖ Found {len(audiobooks)} top audiobooks in {genre}")
            else:
                logger.info(f"‚ÑπÔ∏è Using demo data for {genre}")
                audiobooks = self._get_demo_audiobooks_for_genre(genre)
                        
            return audiobooks
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Scraping failed for {genre}, using demo data: {e}")
            return self._get_demo_audiobooks_for_genre(genre)

    def _get_demo_audiobooks_for_genre(self, genre: str) -> List[Dict]:
        """Get demo audiobooks for a specific genre."""
        # Filter demo audiobooks by genre when possible
        demo_books = []
        for book in self.DEMO_AUDIOBOOKS:
            if any(genre_word.lower() in book['title'].lower() or genre_word.lower() in book['genre'].lower() 
                   for genre_word in genre.split('/')):
                demo_books.append({
                    'title': book['title'],
                    'magnet': 'magnet:?xt=urn:btih:demo-magnet-link',
                    'seeders': book['seeders'],
                    'genre': genre,
                    'size': book['size'],
                    'year': book['year'],
                    'demo': True
                })
        
        # If no matches, return some general demo books
        if not demo_books:
            demo_books = [
                {
                    'title': f"Top {genre} Audiobook #1",
                    'magnet': 'magnet:?xt=urn:btih:demo-magnet-link',
                    'seeders': random.randint(500, 1200),
                    'genre': genre,
                    'size': f"{random.randint(400, 700)} MB",
                    'year': random.randint(2018, 2023),
                    'demo': True
                },
                {
                    'title': f"Top {genre} Audiobook #2",
                    'magnet': 'magnet:?xt=urn:btih:demo-magnet-link',
                    'seeders': random.randint(400, 1000),
                    'genre': genre,
                    'size': f"{random.randint(350, 650)} MB",
                    'year': random.randint(2017, 2022),
                    'demo': True
                }
            ]
        
        return demo_books

    async def find_top_10_audiobooks(self) -> List[Dict]:
        """Find the top 10 audiobooks across popular genres."""
        logger.info("üöÄ Starting comprehensive audiobook search...")
        logger.info("üåê Using VPN-protected scraping with ProtonVPN")
        
        all_audiobooks = []
        
        # Check if we can initialize the scraper
        scraper_available = await self.initialize_scraper()
        
        # Scrape each popular genre
        for genre, category in self.POPULAR_GENRES.items():
            audiobooks = await self.scrape_genre_top_audiobooks(genre, category)
            all_audiobooks.extend(audiobooks)
            
            # Random delay to avoid detection
            await asyncio.sleep(random.uniform(2, 5))
        
        # Sort by seeders and get top 10
        top_audiobooks = sorted(all_audiobooks, key=lambda x: x.get('seeders', 0), reverse=True)[:10]
        
        # Mark demo books
        for book in top_audiobooks:
            book['is_demo'] = book.get('demo', False)
        
        logger.info(f"üèÜ TOP 10 AUDIOBOOKS IDENTIFIED:")
        logger.info("=" * 80)
        for i, book in enumerate(top_audiobooks, 1):
            demo_mark = "üîß" if book.get('demo') else "üåê"
            logger.info(f"{demo_mark} {i:2d}. {book['title'][:70]:<70} ({book.get('seeders', 0)} seeds)")
        
        return top_audiobooks

    def save_results(self, audiobooks: List[Dict]) -> str:
        """Save the results to a JSON file."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = f"top_10_audiobooks_{timestamp}.json"
        
        # Add metadata
        results = {
            'timestamp': datetime.now().isoformat(),
            'total_found': len(audiobooks),
            'scraping_method': 'VPN-protected web scraping' if any(not book.get('demo') for book in audiobooks) else 'Demo data',
            'audiobooks': audiobooks
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        return results_file

    def create_download_commands(self, audiobooks: List[Dict]) -> str:
        """Create manual download commands for each audiobook."""
        commands = []
        commands.append("# MANUAL DOWNLOAD COMMANDS")
        commands.append("# Copy these magnet links to your torrent client:")
        commands.append("")
        
        for i, audiobook in enumerate(audiobooks, 1):
            commands.append(f"{i:2d}. {audiobook['title'][:60]}")
            commands.append(f"    Genre: {audiobook['genre']}")
            commands.append(f"    Seeds: {audiobook.get('seeders', 'N/A')}")
            commands.append(f"    Magnet: {audiobook['magnet']}")
            if audiobook.get('size'):
                commands.append(f"    Size: {audiobook['size']}")
            commands.append("")
        
        return "\n".join(commands)

    async def cleanup(self):
        """Clean up resources."""
        try:
            if self.crawler:
                await self.crawler.__aexit__(None, None, None)
            logger.info("üßπ Cleanup completed")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Cleanup warning: {e}")

    async def run(self):
        """Main execution flow."""
        start_time = datetime.now()
        logger.info("=" * 80)
        logger.info("üéµ TOP 10 AUDIOBOOK FINDER - DEMO")
        logger.info("=" * 80)
        logger.info("üåê VPN Protection: ProtonVPN Active")
        logger.info("üîç Method: Multi-genre web scraping")
        logger.info("üéØ Target: Most popular audiobooks")
        logger.info("=" * 80)
        
        try:
            # Find top audiobooks
            top_audiobooks = await self.find_top_10_audiobooks()
            
            if not top_audiobooks:
                logger.warning("‚ö†Ô∏è No audiobooks found")
                return
            
            # Save results
            results_file = self.save_results(top_audiobooks)
            logger.info(f"üíæ Results saved to: {results_file}")
            
            # Create download commands
            download_commands = self.create_download_commands(top_audiobooks)
            commands_file = f"download_commands_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            with open(commands_file, 'w', encoding='utf-8') as f:
                f.write(download_commands)
            logger.info(f"üìã Download commands saved to: {commands_file}")
            
            # Summary statistics
            end_time = datetime.now()
            duration = end_time - start_time
            demo_count = sum(1 for book in top_audiobooks if book.get('demo'))
            real_count = len(top_audiobooks) - demo_count
            
            logger.info("=" * 80)
            logger.info("üéâ TOP 10 AUDIOBOOK FINDER - COMPLETED")
            logger.info("=" * 80)
            logger.info(f"üìä Total Found: {len(top_audiobooks)} audiobooks")
            logger.info(f"üåê Real Results: {real_count} audiobooks")
            logger.info(f"üîß Demo Data: {demo_count} audiobooks")
            logger.info(f"‚è±Ô∏è Duration: {duration}")
            logger.info(f"üõ°Ô∏è VPN Status: ProtonVPN Protected")
            logger.info("=" * 80)
            logger.info("üìã NEXT STEPS:")
            logger.info("1. Review the JSON results file")
            logger.info("2. Use the magnet links in your torrent client")
            logger.info("3. Consider enabling qBittorrent for automated downloads")
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"‚ùå Fatal error: {e}")
            raise
        finally:
            await self.cleanup()


async def main():
    """Main entry point."""
    try:
        finder = Top10AudiobookFinder()
        await finder.run()
    except KeyboardInterrupt:
        logger.info("üõë Operation cancelled by user")
    except Exception as e:
        logger.error(f"‚ùå Application error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())