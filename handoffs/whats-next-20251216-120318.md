<original_task>
General project maintenance, security hardening, and status check.
Specifically: Finishing "Tunnel to Production", but then pivoting to "Data Unification" after VPN approach was descoped.
</original_task>

<work_completed>
- **Strategic Pivot:**
  - **Decision:** The "Dual VPN" architecture (running one scraper on VPN, one on ISP) was blocking progress due to Windows routing complexity.
  - **Action:** User requested to drop this requirement. Issue `MAMcrawler-cpe` was **CLOSED**.
  - **Implication:** The project is now simpler. We rely on the single connection or basic VPN without complex split tunneling for now.

- **Security Hardening (Completed Previously):**
  - Secrets removed, Rate Limits applied. This is done.

- **Status Check & Audit:**
  - Confirmed that the "Data Silo" is the main technical debt.
  - `DiscoveryService` and `MAMSeleniumService` operate on `audiobooks_to_download.json`.
  - The API/Dashboard operates on PostgreSQL `books` table.
  - **Mismatch:** The Dashboard is "blind" to the actual crawler activity because they don't share a data store.

- **Task Management:**
  - Updated Epic `MAMcrawler-0dm` to show VPN tasks as done/descoped.
  - Identified next task `MAMcrawler-du1` (Data Unification) but did not create it in `beads` yet to avoid cluttering the board before handoff.
</work_completed>

<work_remaining>
- **Create & Execute Task: Data Unification (`MAMcrawler-du1`):**
  - **Goal:** Make `DiscoveryService` and `MAMSeleniumService` read/write directly to PostgreSQL.
  - **Step 1:** Modify `backend/services/discovery_service.py` -> `load_download_list` is already reading from DB, but *where does it write*? It writes to `audiobooks_to_download.json` (implicitly, via other scripts). Need to check `execute_real_workflow...` scripts.
  - **Step 2:** Ensure `MAMSeleniumService` updates the `Download` table status (currently it tries, but error handling is weak).
  - **Step 3:** Eliminate the JSON file middleware entirely.

- **Revive "Orphaned" Sync:**
  - Wrap `audiobookshelf_hardcover_sync.py` into a proper Service class and call it from the API.
</work_remaining>

<attempted_approaches>
- **JSON Migration:** Tried running `backend/scripts/migrate_json_to_db.py`.
  - **Result:** Failed because `audiobooks_to_download.json` was missing.
  - **Meaning:** The queue might be empty, or the file location is different in production. Since we are moving to DB anyway, we might just start fresh or rescan.
</attempted_approaches>

<critical_context>
- **Architecture Change:** NO MORE DUAL VPN. Do not try to fix `verify_wireguard.py` or debug routing tables. That path is abandoned.
- **Current Blocker:** The dashboard works, but data doesn't flow to it from the crawler.
- **Environment:** Windows 10/11. Python 3.11. `beads` CLI active.
</critical_context>

<current_state>
- **VPN:** Descoped/Closed.
- **Security:** Green.
- **Data Flow:** Broken (Fragmented).
- **Next Logic Step:** Connect the pipes. Make the Crawler talk to the DB.
</current_state>
