<original_task>
Data Unification: Refactor DiscoveryService and MAMSeleniumService to read/write directly to PostgreSQL and enable the "Download Loop" via the backend scheduler.
</original_task>

<work_completed>
- **Implemented Data Unification Loop:**
  - **Modified `DiscoveryService` (`backend/services/discovery_service.py`):** Added `queue_downloads` method to write found books directly to the `Download` table in PostgreSQL.
  - **Created New Scheduler Task (`backend/schedulers/tasks.py`):** Implemented `process_download_queue_task` which:
    1. Calls `DiscoveryService.find_new_books` to identify gaps.
    2. Calls `DiscoveryService.queue_downloads` to persist them.
    3. Calls `MAMSeleniumService.run_search_and_download` with items from the DB queue.
  - **Registered Task (`backend/schedulers/register_tasks.py`):** Scheduled `process_download_queue` to run every 30 minutes.

- **Status Management:**
  - Closed `MAMcrawler-oip` (Data Unification) as "implemented".
  - Confirmed `MAMcrawler-cpe` (Dual VPN) is closed/descoped.

</work_completed>

<work_remaining>
- **Restart Backend:**
  - The `MAMcrawler` backend service (Uvicorn) MUST be restarted for the new scheduler tasks to load.
- **Verification:**
  - Watch the logs (`backend.log` or standard out) for "Starting download queue processing task".
  - Verify that new items appear in the Dashboard's "Downloads" section automatically.
- **Cleanup (Optional):**
  - Delete `audiobooks_to_download.json` if it reappears (it shouldn't).
  - Archive/Delete legacy scripts like `execute_real_workflow_final_real.py` once the scheduler is proven to work reliable.
- **Orphaned Sync (Next Logic Task):**
  - Integrate `audiobookshelf_hardcover_sync.py` into `backend/services/sync_service.py` to complete the loop.
</work_remaining>

<attempted_approaches>
- **JSON Migration:** Previous attempt to migrate JSON to DB failed because the file was missing. This is now moot as the new system re-populates from the source (Library Scan).
</attempted_approaches>

<critical_context>
- **No More JSON:** The system is now designed to be Database-Centric. The "Data Silo" where the dashboard couldn't see the crawler's work is resolved *architecturally*.
- **Scheduler Dependency:** The entire automation now relies on `backend/schedulers/scheduler.py` running. Ensure `SCHEDULER_ENABLED=true` in `.env` (default is usually true).
</critical_context>

<current_state>
- **Codebase:** Refactored for DB-centric flow.
- **Deployment:** Pending Restart.
- **Next Step:** Restart server -> Verify Logs -> Celebrate.
</current_state>
